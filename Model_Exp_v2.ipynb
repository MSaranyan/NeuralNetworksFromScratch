{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduction=16):\n",
    "        super().__init__()\n",
    "        self.maxpool=nn.AdaptiveMaxPool1d(1)\n",
    "        self.avgpool=nn.AdaptiveAvgPool1d(1)\n",
    "        self.se=nn.Sequential(\n",
    "            nn.Conv1d(channel,channel//reduction,1,bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channel//reduction,channel,1,bias=False)\n",
    "        )\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        max_result=self.maxpool(x)\n",
    "        avg_result=self.avgpool(x)\n",
    "        max_out=self.se(max_result)\n",
    "        avg_out=self.se(avg_result)\n",
    "        output=self.sigmoid(max_out+avg_out)\n",
    "        return output\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self,kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Conv1d(2,1,kernel_size=kernel_size,padding=kernel_size//2)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        max_result,_=torch.max(x,dim=1,keepdim=True)\n",
    "        avg_result=torch.mean(x,dim=1,keepdim=True)\n",
    "        result=torch.cat([max_result,avg_result],1)\n",
    "        output=self.conv(result)\n",
    "        output=self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channel=512,reduction=16,kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ca=ChannelAttention(channel=channel,reduction=reduction)\n",
    "        self.sa=SpatialAttention(kernel_size=kernel_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        residual=x\n",
    "\n",
    "        out_ca = x*self.ca(x)\n",
    "        out_sa = out_ca*self.sa(out_ca)\n",
    "\n",
    "        out = out_sa + residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKUnit(nn.Module):\n",
    "    def __init__(self, CBAMBlock, kernels=[3,5,7,11], channels=None, L = 32, reduction=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d=max(L,channels//reduction)\n",
    "        \n",
    "        self.path_s = nn.ModuleList(\n",
    "            [self._make_pathways(kernel_size=kernels[i],\n",
    "            stride=2, channels=channels, CBAMBlock=CBAMBlock) for i in range(len(kernels))]\n",
    "        ) \n",
    "        \n",
    "        \n",
    "        self.up_s = nn.ModuleList(\n",
    "            [\n",
    "                nn.ConvTranspose1d(in_channels=channels, out_channels=channels,kernel_size=kernels[i],\n",
    "                                        stride=2) for i in range(len(kernels))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc=nn.Linear(channels,self.d)\n",
    "\n",
    "        self.fcs=nn.ModuleList([])\n",
    "        for i in range(len(kernels)):\n",
    "            self.fcs.append(nn.Linear(self.d,channels))\n",
    "\n",
    "        self.softmax=nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        identity = input\n",
    "        bs, c, _, = input.size()\n",
    "\n",
    "        p_s = []\n",
    "        for layer in self.path_s:\n",
    "            p_s.append(layer(input))\n",
    "\n",
    "        x_s = []\n",
    "        for idx, layer in enumerate(self.up_s):\n",
    "            x_s.append(layer(p_s[idx], output_size=identity.size()))\n",
    "\n",
    "        print(f\"p_shape{p_s[0].shape}, x_0.shape{x_s[0].shape}\")\n",
    "\n",
    "        feats = torch.stack(x_s, dim=0)\n",
    "        print(f\"feats.shape{feats.shape}\")\n",
    "        select = torch.sum(feats, dim=0)\n",
    "        print(f\"select.shape{select.shape}\")\n",
    "        S=select.mean(-1).mean(-1) \n",
    "        print(f\"S.shape:{S.shape}\")\n",
    "        Z=self.fc(S) \n",
    "\n",
    "        weights=[]\n",
    "        for fc in self.fcs:\n",
    "            weight=fc(Z)\n",
    "            weights.append(weight.view(bs,c,1)) \n",
    "        attention_weughts=torch.stack(weights,0)\n",
    "        attention_weughts=self.softmax(attention_weughts)\n",
    "\n",
    "        V=(attention_weughts*feats).sum(0)\n",
    "        return V\n",
    "\n",
    "    def _make_pathways(self, kernel_size, stride, channels, CBAMBlock):\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=stride),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.ReLU(),\n",
    "            CBAMBlock(channels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_shapetorch.Size([2, 64, 1998]), x_0.shapetorch.Size([2, 64, 4000])\n",
      "feats.shapetorch.Size([3, 2, 64, 4000])\n",
      "select.shapetorch.Size([2, 64, 4000])\n",
      "S.shape:torch.Size([2])\n"
     ]
    },
   ],
   "source": [
    "sk = SKUnit(CBAMBlock, channels=64, kernels=[5,7,11])\n",
    "x  = torch.randn(2,64,4000)\n",
    "output = sk(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(self, SKUnit, CBAMBlock, kernels, in_channels, out_channels, identity_downsample=None, stride = 1):\n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 2\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.conv2 = SKUnit(CBAMBlock, channels=out_channels, kernels=kernels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels*self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        \n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, SKUnit, CBAMBlock, layers, num_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.in_channels = 96\n",
    "        self.expansion = 2\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_channels, out_channels=self.in_channels,\n",
    "                               kernel_size=320, stride=8, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(self.in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.kernels = [\n",
    "            [5,7,11],\n",
    "            [3,5,7],\n",
    "            [3,5],\n",
    "            [3,5]\n",
    "        ]\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, layers[0], self.kernels[0], out_channels=96, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], self.kernels[1], out_channels=192, stride=1)\n",
    "        self.layer3 = self._make_layer(block, layers[2], self.kernels[2], out_channels=384, stride=1)\n",
    "        self.layer4 = self._make_layer(block, layers[3], self.kernels[3], out_channels=384, stride=1)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d((1,1))\n",
    "        self.fc = nn.Linear(self.in_channels, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, no_residual_blocks, kernels, out_channels, stride):\n",
    "\n",
    "        layers = []\n",
    "        identity_downsample = None\n",
    "\n",
    "        if stride != 1 or self.in_channels != out_channels*self.expansion:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels=out_channels*self.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(self.expansion*out_channels)\n",
    "            )\n",
    "        \n",
    "        layers.append(block(SKUnit, CBAMBlock, kernels, self.in_channels, out_channels, identity_downsample, stride))\n",
    "        self.in_channels = out_channels*self.expansion\n",
    "\n",
    "        for i in range(no_residual_blocks-1):\n",
    "            layers.append(block(SKUnit, CBAMBlock, kernels, self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2,2,2,2]\n",
    "model = ResNet(block,SKUnit, CBAMBlock, layers, num_channels=1, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_shapetorch.Size([2, 96, 62, 60]), x_0.shapetorch.Size([2, 96, 128, 124])\n",
      "feats.shapetorch.Size([3, 2, 96, 128, 124])\n",
      "select.shapetorch.Size([2, 96, 128, 124])\n",
      "S.shape:torch.Size([2, 96])\n",
      "p_shapetorch.Size([2, 96, 62, 60]), x_0.shapetorch.Size([2, 96, 128, 124])\n",
      "feats.shapetorch.Size([3, 2, 96, 128, 124])\n",
      "select.shapetorch.Size([2, 96, 128, 124])\n",
      "S.shape:torch.Size([2, 96])\n",
      "p_shapetorch.Size([2, 192, 31, 30]), x_0.shapetorch.Size([2, 192, 64, 62])\n",
      "feats.shapetorch.Size([3, 2, 192, 64, 62])\n",
      "select.shapetorch.Size([2, 192, 64, 62])\n",
      "S.shape:torch.Size([2, 192])\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (1,512, 497))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fyp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fd12c0110a4e677020ed062760c440d27ea6b6f2cfce5522b10596f8f3a8860"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
