{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduction=16):\n",
    "        super().__init__()\n",
    "        self.maxpool=nn.AdaptiveMaxPool2d(1)\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(1)\n",
    "        self.se=nn.Sequential(\n",
    "            nn.Conv2d(channel,channel//reduction,1,bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channel//reduction,channel,1,bias=False)\n",
    "        )\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        max_result=self.maxpool(x)\n",
    "        avg_result=self.avgpool(x)\n",
    "        max_out=self.se(max_result)\n",
    "        avg_out=self.se(avg_result)\n",
    "        output=self.sigmoid(max_out+avg_out)\n",
    "        return output\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self,kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Conv2d(2,1,kernel_size=kernel_size,padding=kernel_size//2)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        max_result,_=torch.max(x,dim=1,keepdim=True)\n",
    "        avg_result=torch.mean(x,dim=1,keepdim=True)\n",
    "        result=torch.cat([max_result,avg_result],1)\n",
    "        output=self.conv(result)\n",
    "        output=self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channel=512,reduction=16,kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ca=ChannelAttention(channel=channel,reduction=reduction)\n",
    "        self.sa=SpatialAttention(kernel_size=kernel_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        residual=x\n",
    "\n",
    "        out_ca = x*self.ca(x)\n",
    "        out_sa = out_ca*self.sa(out_ca)\n",
    "\n",
    "        out = out_sa + residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "cbam = CBAMBlock(channel=64)\n",
    "x  = torch.randn(2,64,7,7)\n",
    "output = cbam(x)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKUnit(nn.Module):\n",
    "    def __init__(self, CBAMBlock, kernels=[3,5,7,11], channels=None, L = 32, reduction=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d=max(L,channels//reduction)\n",
    "        \n",
    "        self.path_0 = self._make_pathways(kernel_size=(kernels[0], kernels[0]),\n",
    "                                        stride=2, channels=channels, CBAMBlock=CBAMBlock)\n",
    "        self.path_1 = self._make_pathways(kernel_size=(kernels[1], kernels[1]),\n",
    "                                        stride=2, channels=channels, CBAMBlock=CBAMBlock)\n",
    "        self.path_2 = self._make_pathways(kernel_size=(kernels[2], kernels[2]),\n",
    "                                        stride=2, channels=channels, CBAMBlock=CBAMBlock)\n",
    "        self.path_3 = self._make_pathways(kernel_size=(kernels[3], kernels[3]),\n",
    "                                        stride=2, channels=channels, CBAMBlock=CBAMBlock)\n",
    "        \n",
    "        self.up_0 = nn.ConvTranspose2d(in_channels=channels, out_channels=channels,kernel_size=(kernels[0], kernels[0]),\n",
    "                                        stride=2)\n",
    "        self.up_1 = nn.ConvTranspose2d(in_channels=channels, out_channels=channels,kernel_size=(kernels[1], kernels[1]),\n",
    "                                        stride=2)\n",
    "        self.up_2 = nn.ConvTranspose2d(in_channels=channels, out_channels=channels,kernel_size=(kernels[2], kernels[2]),\n",
    "                                        stride=2)\n",
    "        self.up_3 = nn.ConvTranspose2d(in_channels=channels, out_channels=channels,kernel_size=(kernels[3], kernels[3]),\n",
    "                                        stride=2)\n",
    "\n",
    "        self.fc=nn.Linear(channels,self.d)\n",
    "\n",
    "        self.fcs=nn.ModuleList([])\n",
    "        for i in range(len(kernels)):\n",
    "            self.fcs.append(nn.Linear(self.d,channels))\n",
    "\n",
    "        self.softmax=nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        identity = input\n",
    "        bs, c, _, _ = input.size()\n",
    "\n",
    "        p_0 = self.path_0(input)\n",
    "        p_1 = self.path_1(input)\n",
    "        p_2 = self.path_2(input)\n",
    "        p_3 = self.path_3(input)\n",
    "\n",
    "        x_0 = self.up_0(p_0, output_size=identity.size())\n",
    "        x_1 = self.up_1(p_1, output_size=identity.size())\n",
    "        x_2 = self.up_2(p_2, output_size=identity.size())\n",
    "        x_3 = self.up_3(p_3, output_size=identity.size())\n",
    "\n",
    "        print(f\"p_shape{p_0.shape}, x_0.shape{x_0.shape}\")\n",
    "\n",
    "        feats = torch.stack([x_0, x_1, x_2, x_3], dim=0)\n",
    "        print(f\"feats.shape{feats.shape}\")\n",
    "        select = torch.sum(feats, dim=0)\n",
    "        print(f\"select.shape{select.shape}\")\n",
    "        S=select.mean(-1).mean(-1) \n",
    "        print(f\"S.shape:{S.shape}\")\n",
    "        Z=self.fc(S) \n",
    "\n",
    "        weights=[]\n",
    "        for fc in self.fcs:\n",
    "            weight=fc(Z)\n",
    "            weights.append(weight.view(bs,c,1,1)) \n",
    "        attention_weughts=torch.stack(weights,0)\n",
    "        attention_weughts=self.softmax(attention_weughts)\n",
    "\n",
    "        V=(attention_weughts*feats).sum(0)\n",
    "        return V\n",
    "\n",
    "    def _make_pathways(self, kernel_size, stride, channels, CBAMBlock):\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=stride),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(),\n",
    "            CBAMBlock(channels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_shapetorch.Size([2, 64, 55, 55]), x_0.shapetorch.Size([2, 64, 112, 112])\n",
      "feats.shapetorch.Size([4, 2, 64, 112, 112])\n",
      "select.shapetorch.Size([2, 64, 112, 112])\n",
      "S.shape:torch.Size([2, 64])\n",
      "torch.Size([2, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "sk = SKUnit(CBAMBlock, channels=64)\n",
    "x  = torch.randn(2,64,112,112)\n",
    "output = sk(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(self, SKUnit, CBAMBlock, in_channels, out_channels, identity_downsample=None, stride = 1):\n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = SKUnit(CBAMBlock, channels=out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        \n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, SKUnit, CBAMBlock, layers, num_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.expansion = 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=self.in_channels,\n",
    "                               kernel_size=3, stride=2, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride=1)\n",
    "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride=1)\n",
    "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride=1)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(self.in_channels, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, no_residual_blocks, out_channels, stride):\n",
    "\n",
    "        layers = []\n",
    "        identity_downsample = None\n",
    "\n",
    "        if stride != 1 or self.in_channels != out_channels*self.expansion:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels=out_channels*self.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(self.expansion*out_channels)\n",
    "            )\n",
    "        \n",
    "        layers.append(block(SKUnit, CBAMBlock, self.in_channels, out_channels, identity_downsample, stride))\n",
    "        self.in_channels = out_channels*self.expansion\n",
    "\n",
    "        for i in range(no_residual_blocks-1):\n",
    "            layers.append(block(SKUnit, CBAMBlock, self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2,2,2,2]\n",
    "model = ResNet(block,SKUnit, CBAMBlock, layers, num_channels=1, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fyp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fd12c0110a4e677020ed062760c440d27ea6b6f2cfce5522b10596f8f3a8860"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
